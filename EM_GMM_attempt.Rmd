---
title: "Reproducing Online Report of EM"
author: "R. Noah Padgett"
date: "2019-04-15"
output: html_document
---

## Page Setup

This file is an extension of the previous papge where I reproduced the code given by others.
This time, my goal is to expand on how to compute the different parameters and generalize the code a little to allow for more complex analyses. 
Though, the code is still very simple (univariate mixtures).

# Example

In this example, we will assume our mixture components are a mixture of two unknown Gaussian distributions (i.e the means and variances are unknown), and we are interested in finding the maximum likelihood estimates of the $\pi_k, \mu_k, \sigma^2_k$â€™s.

Assume we have $K=2$ components, so that:
\[
X_i | Z_i = 0 \sim N(5,1.5)\\
X_i | Z_i = 1 \sim N(10,2)
\]

The true mixture proportions will be $P(Z_i=0)=0.25$ and $P(Z_i=1)=0.75$. First we simulate data from this mixture model:

```{r}
set.seed(12345)
## Total Sample Size
N <- 10000

## mixture components
mu.true    <- c(5, 10) ## Mean vector
sigma.true <- c(1.5, 2) ## SD vector

## determine Z_i: class assignment
Z <- rbinom(N, 1, 0.75)

## sample from univariate mixture model
X <- rnorm(N)*sigma.true[Z+1]+mu.true[Z+1]
hist(X)

```

Now, we weite a function to compute the log-likelihood for the incomplete data, assuming the parameters are known. 
This log-likelihood calculator will be used to determine convergence:

\[
l(\theta) = \sum_{i=1}^n\log \left(\sum_{k=1}^K\pi_k N(x_i; \mu_k, \sigma^2_k)\right)
\]
where $N(x_i; \mu_k, \sigma^2_k)$ is the likelihood of the $i^{th}$ observation in the $k^{th}$ class (or mixture).
The function is written as:

```{r}

compute.log.lik <- function(X, theta, K){
  # Calculate indivial class probabilities
  L <- matrix(nrow=length(X),ncol=K)
  i <- 1 ## counter for parametes
  for(k in 1:K){
    L[,k] <- dnorm(X, mean=theta[1,k], sd = theta[2,k])*theta[3,k]
  } ## end loop around computing log-likelihoods
  ## compute sum of the log-likelihoods across classes
  ll <- sum(log(rowSums(L)))
  return(ll)
}

```

which computes the log-likelihood based on the current estimates of mean, variance, and mixing weights.
Finally, we implement the E and M step in the EM.iter function below. 
The mixture.EM function is the driver which checks for convergence by computing the log-likelihoods at each step.

```{r}

mixture.EM <- function(X, K, max.iter = 1000) {
  ## initial values for parameters
  # order of parameters is
  # mu, sigma, pi, mu,sigma, pi, etc..
  ## test out
  t.init   <- matrix(rep(c(0,1,1/K),K), ncol=K)
  # jitter start means
  t.init.j <- matrix(c(sample(X, K),  ## sample random points for means
                abs(rnorm(K, mean = sd(X), sd = sd(X)/2)),  # use sd as center for generating random SDs for each class
                rep(0,K)), ## don't jitter pi
                byrow=T, ncol=K)
  ## Combine after jitter
  t.curr   <- t.init + t.init.j
  ## store log-likehoods for each iteration
  log_liks <- c()
  ll       <- compute.log.lik(X, t.curr, K)
  log_liks <- c(log_liks, ll)
  delta.ll <- 1
  ## saving ALL parameter estimates
  t.array <- array(dim =c(3,2,max.iter))
  t.array[,,1] <- t.curr ## initial values
  
  ## Run the model until condition is met
  iter <- 2
  while(delta.ll > 1e-5){
    ## run E + M seps based on current iteration
    t.curr   <- EM.iter(t.curr, K)
    # update stored estimates
    t.array[,,iter] <- t.curr
    ##compute next LL
    ll       <- compute.log.lik(X, t.curr, K)
    ## update LL stored
    log_liks <- c(log_liks, ll) 
    ## next, calculate the change in LL between iterations
    delta.ll <- log_liks[length(log_liks)]  - log_liks[length(log_liks)-1]
    iter <- iter + 1
    if(iter == max.iter) break
  }
  colnames(t.curr) <- paste('Class',1:K)
  rownames(t.curr) <- c("Mu", "Sigma", "Pi")
  
  t.array <- t.array[,,1:length(log_liks)]
  
  return(list(t.curr, log_liks, t.array))
}

EM.iter <- function(t.curr, K, ...) {
  ## initialize the next iteration values
  t.next <- t.curr
  
  ## E-step: compute E_{Z|X,w0}[I(Z_i = k)]
  # Calc Posterior class probabilities
  z_ik <- matrix(nrow=length(X),ncol=K)
  for(k in 1:K){
     z_ik[,k] <- dnorm(X, mean=t.curr[1,k], sd = t.curr[2,k])*t.curr[3,k]
  } ## end loop around computing log-likelihoods
  z_ik <-  z_ik / rowSums( z_ik)

  ## M-Step
  # update class means estimate
  for(k in 1:K){
    t.next[1,k] <- sum(z_ik[,k]*X)/sum(z_ik[,k])
  } ## end loop around mean update
  
  # update class std estimates
  for(k in 1:K){
    t.next[2,k] <- sqrt(sum(z_ik[,k]*(X - t.next[1,k])**2)/sum(z_ik[,k]))
  } ## end loop around std update
  
  # update class weight estimates
  t.next[3,] <- colSums(z_ik)/sum(z_ik) 
  return(t.next)
}

```

Next, try to run the EM algorithm based on how to compute the different pieces. 

```{r}

## perform EM
# X = observed data
# L = likelihood of class memberships
ee <- mixture.EM(X, K=2)
ee[[1]]

```

Finally, we inspect the evolution of the log-likelihood and note that it is strictly increases:

```{r}

plot(ee[[2]][-1], ylab='incomplete log-likelihood', xlab='iteration')


## mean as a function of log-like
plot(ee[[3]][1,1,],ee[[2]],  ylab='log-likelihood', xlab='mean of class 1')
plot(ee[[3]][1,2,],ee[[2]],  ylab='log-likelihood', xlab='mean of class 2')


plot(ee[[3]][1,1,],  ylab='mean of class 1', xlab="iteration")
```







